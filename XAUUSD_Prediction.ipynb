# %% [markdown]
# # XAU/USD Price Prediction with Proper Scaling

# %%
# Cell 1: Import libraries
import os
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf
from datetime import datetime, timedelta
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import pickle

# TensorFlow imports
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.metrics import MeanSquaredError

np.random.seed(42)

# %%
# Cell 2: Data Collection with Price Anchoring
def fetch_data():
    print("Fetching historical data...")
    end_date = datetime.now()
    start_date = end_date - timedelta(days=3*365)  # 3 years of data
    
    try:
        # Get current market price to anchor our data
        current_data = yf.download("GC=F", period="1d")
        current_price = current_data['Close'].iloc[0]
        print(f"Current XAU/USD price: ${current_price:.2f}")
        
        # Fetch historical data
        data = yf.download("GC=F", start=start_date, end=end_date, progress=False)
        
        # If empty, create synthetic data around current price
        if data.empty:
            print("Creating synthetic data around current price")
            dates = pd.date_range(start=start_date, end=end_date, freq='D')
            base_price = current_price * np.random.uniform(0.95, 1.05)  # ±5% of current
            close_prices = base_price + np.cumsum(np.random.normal(0, 5, len(dates)))
            data = pd.DataFrame({
                'Close': close_prices,
                'High': close_prices + np.random.uniform(1, 10, len(dates)),
                'Low': close_prices - np.random.uniform(1, 10, len(dates)),
                'Open': close_prices + np.random.uniform(-5, 5, len(dates)),
                'Volume': np.random.randint(10000, 50000, len(dates))
            }, index=dates)
        
        # Ensure prices are realistic
        data['Close'] = data['Close'].clip(current_price*0.8, current_price*1.2)
        
        # Save data
        os.makedirs('data', exist_ok=True)
        data.to_csv('data/xauusd_historical.csv')
        return data[['Close', 'High', 'Low', 'Open', 'Volume']]
    
    except Exception as e:
        print(f"Error fetching data: {e}")
        # Fallback to synthetic data around $3300
        print("Using fallback synthetic data")
        dates = pd.date_range(end=end_date, periods=3*365, freq='D')
        base_price = 3300 + np.random.normal(0, 50)
        close_prices = base_price + np.cumsum(np.random.normal(0, 5, len(dates)))
        data = pd.DataFrame({
            'Close': close_prices,
            'High': close_prices + np.random.uniform(5, 15, len(dates)),
            'Low': close_prices - np.random.uniform(5, 15, len(dates)),
            'Open': close_prices + np.random.uniform(-10, 10, len(dates)),
            'Volume': np.random.randint(10000, 50000, len(dates))
        }, index=dates)
        data.to_csv('data/xauusd_historical.csv')
        return data

data = fetch_data()
print("\nFirst 5 rows:")
print(data.head())
print("\nLast 5 rows:")
print(data.tail())

# %%
# Cell 3: Feature Engineering with Price-Consistent Features
def add_features(df):
    df = df.copy()
    
    # Technical indicators
    df['SMA_10'] = df['Close'].rolling(10).mean()
    df['SMA_50'] = df['Close'].rolling(50).mean()
    df['Daily_Return'] = df['Close'].pct_change()
    df['Volatility'] = df['Close'].rolling(5).std()
    
    # Price change features
    df['Price_Change_1D'] = df['Close'].diff()
    df['Price_Change_3D'] = df['Close'].diff(3)
    
    # Drop NA values
    df = df.dropna()
    return df

data = add_features(data)
print("\nData with features:")
print(data.tail())

# %%
# Cell 4: Proper Data Scaling
def prepare_data(df, lookback=30):
    # Create target (next day's close)
    df['Target'] = df['Close'].shift(-1)
    df = df.dropna()
    
    # Select features (exclude target and non-numeric)
    feature_cols = [col for col in df.columns 
                   if col != 'Target' and pd.api.types.is_numeric_dtype(df[col])]
    
    # Scale features separately from target
    feature_scaler = MinMaxScaler()
    features = feature_scaler.fit_transform(df[feature_cols])
    
    # Scale target separately
    target_scaler = MinMaxScaler()
    target = target_scaler.fit_transform(df[['Target']])
    
    # Save scalers
    os.makedirs('models', exist_ok=True)
    with open('models/feature_scaler.pkl', 'wb') as f:
        pickle.dump(feature_scaler, f)
    with open('models/target_scaler.pkl', 'wb') as f:
        pickle.dump(target_scaler, f)
    
    # Create sequences
    X, y = [], []
    for i in range(len(features) - lookback):
        X.append(features[i:i+lookback])
        y.append(target[i+lookback][0])  # Get scalar value
    
    X = np.array(X)
    y = np.array(y)
    
    # Train-test split
    split = int(0.8 * len(X))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]
    
    return X_train, X_test, y_train, y_test, feature_cols

X_train, X_test, y_train, y_test, feature_cols = prepare_data(data)
print(f"\nTraining shape: {X_train.shape}, Test shape: {X_test.shape}")

# %%
# Cell 5: Model Building with Proper Output Scaling
def build_model(input_shape):
    model = Sequential([
        LSTM(64, return_sequences=True, input_shape=input_shape),
        Dropout(0.3),
        LSTM(32),
        Dropout(0.3),
        Dense(16, activation='relu'),
        Dense(1, activation='linear')  # Linear activation for regression
    ])
    model.compile(optimizer='adam', loss='mse', metrics=[MeanSquaredError()])
    return model

model = build_model((X_train.shape[1], X_train.shape[2]))
model.summary()

# %%
# Cell 6: Model Training
callbacks = [
    EarlyStopping(patience=10, restore_best_weights=True),
    ModelCheckpoint('models/best_model.keras', save_best_only=True)
]

print("\nTraining model...")
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=32,
    callbacks=callbacks,
    verbose=1
)

# Plot training
plt.figure(figsize=(12, 5))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training History')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Load best model
model = load_model('models/best_model.keras')

# %%
# Cell 7: Evaluation with Proper Inverse Scaling
with open('models/target_scaler.pkl', 'rb') as f:
    target_scaler = pickle.load(f)

# Make predictions
test_pred_scaled = model.predict(X_test).flatten()
test_pred = target_scaler.inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()
y_test_actual = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()

# Calculate metrics
test_mae = mean_absolute_error(y_test_actual, test_pred)
print(f"\nTest MAE: ${test_mae:.2f}")

# Plot actual vs predicted
plt.figure(figsize=(14, 6))
plt.plot(data.index[-len(y_test_actual):], y_test_actual, label='Actual', color='blue')
plt.plot(data.index[-len(test_pred):], test_pred, label='Predicted', color='red', linestyle='--')
plt.title('XAU/USD Price Prediction')
plt.xlabel('Date')
plt.ylabel('Price (USD)')
plt.legend()
plt.grid(True)
plt.show()

# %%
# Cell 8: Future Prediction with Correct Scaling
def predict_future(days=5):
    with open('models/feature_scaler.pkl', 'rb') as f:
        feature_scaler = pickle.load(f)
    with open('models/target_scaler.pkl', 'rb') as f:
        target_scaler = pickle.load(f)
    
    lookback = X_train.shape[1]
    last_seq = data[feature_cols].iloc[-lookback:]
    last_scaled = feature_scaler.transform(last_seq)
    
    predictions = []
    current_seq = last_scaled.copy()
    
    for _ in range(days):
        input_data = current_seq.reshape(1, lookback, -1)
        pred_scaled = model.predict(input_data, verbose=0)[0][0]
        pred = target_scaler.inverse_transform([[pred_scaled]])[0][0]
        
        # Ensure realistic price movement (max ±1.5% daily change)
        last_price = data['Close'].iloc[-1] if not predictions else predictions[-1]
        pred = np.clip(pred, last_price*0.985, last_price*1.015)
        predictions.append(pred)
        
        # Update sequence with realistic values
        new_seq = np.roll(current_seq, -1, axis=0)
        new_row = last_scaled[-1:].copy()
        
        # Update the close price in the features (index 0)
        new_row[0, 0] = (pred - target_scaler.min_[0]) / target_scaler.scale_[0]
        new_seq[-1:] = new_row
        current_seq = new_seq
    
    future_dates = [datetime.now() + timedelta(days=i) for i in range(1, days+1)]
    return future_dates, predictions

print("\nMaking realistic predictions...")
try:
    dates, preds = predict_future()
    print("\nNext 5 Day Predictions:")
    for date, pred in zip(dates, preds):
        print(f"{date.strftime('%Y-%m-%d')}: ${pred:.2f}")
    
    # Plot future predictions
    plt.figure(figsize=(14, 6))
    plt.plot(data.index[-30:], data['Close'][-30:], label='Historical Prices', color='blue')
    plt.plot(dates, preds, 'ro-', label='Predicted Prices')
    plt.title('XAU/USD Future Price Prediction')
    plt.xlabel('Date')
    plt.ylabel('Price (USD)')
    plt.legend()
    plt.grid(True)
    plt.show()
except Exception as e:
    print(f"Prediction error: {str(e)}")

# %%
# Cell 9: Save Results
try:
    model.save('models/xauusd_predictor.keras')
    data.to_csv('data/xauusd_latest.csv')
    print("\nAll results saved successfully!")
except Exception as e:
    print(f"Error saving results: {str(e)}")
